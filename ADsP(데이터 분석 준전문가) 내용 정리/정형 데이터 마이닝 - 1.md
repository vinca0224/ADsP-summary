# 정형 데이터 마이닝 - 1

---

- 데이터 마이닝의 기초 개념 학습
- 데이터 분할 학습
- 분류분석 방법과 예측모형 수립방법 학습

---

**데이터 마이닝**이란 방대한 양의 데이터 속에서 숨겨진 규칙, 패턴들을 찾아 예측하거나 의사결정에 활용하는 것이다.

- **목적 정의** : 분석목적과 필요 데이터 정의
- **데이터 준비** : 데이터 수집 및 정제
- **데이터 가공** : 분석 목적에 맞는 변수 정의, 데이터 가공
- **데이터 마이닝 기법 적용** : 분석기법을 적용해 정보 추출
- **검증** : 추출 정보 검증

데이터 마이닝의 종류

1. 방법에 따른 분류
    - 지도학습 : 정답이 있는 데이터를 활용해 분석모델 학습, 독립변수에 따른 종속 변수가 있다.
    - 비지도학습 : 정답을 알려주지 않고 학습, 독립변수에 따른 종속 변수가 있다.
2. 분석 목적에 따른 분류
    - 분류분석 : 데이터가 어느 그룹에 속하는지 판별
    - 군집분석 : 유사성을 측정하여 유사성이 높은 객체끼리 하나의 그룹으로 묶는 분석
    - 연관분석 : 장바구니 분석, 데이터의 연관성 파악

---

데이터 분할 : 데이터 마이닝 적용 전 데이터를 훈련용, 검정용, 평가용으로 분할

- 훈련용 : 모델을 구축
- 검정용 : 구축된 모델이 적합한지 검증, 모델 과대추정과 과소추정 방지
- 평가용 : 최종 구축된 모델의 성능 평가

**과적합**은 모델이 복잡하고 해서이 어려우며, **과소적합**은 데이터를 충분히 설명하지 못한다.

---

데이터 분할을 통한 검증

- **홀드아웃** : 전체 데이터를 **랜덤하게 추출**해 데이터를 분리, 가장 보편적 방법
- **K - Fold 교차검증** : 전체 데이터를 **k개의 집단으로 구분 후, k-1개를 훈련용, 나머지 1개를 평가용으로 사용**
    - 정확도 향상 및 과적합, 과소적합 모두 방지
    - 데이터가 적으면 과적합 방지가 어렵고 모델훈련에 시간이 걸림
        
        ![https://i.imgur.com/9k60cVA.png](https://i.imgur.com/9k60cVA.png)
        
- **붓스트랩** : 원본의 데이터의 크기만큼 **복원추출**을 진행
    - 데이터셋의 분포가 고르지 않을 때 사용
- **계층별 K-겹 교차 검증** : **불균형 데이터**를 분류
    - 각 폴드가 가지는 레이블의 분포가 유사하도록 폴드를 추출해 교차검증
    

**언더샘플링**은 특정 범주가 많은 데이터를 다른 범주와 균형을 맞추도록 데이터 셋을 축소하는 작업이다.

**오버샘플링**은 특정 범주가 작은 데이터를 데이터 셋의 크기를 확장시키는 작업이다.

---

**로지스틱 회귀분석**은 회귀분석을 분류에 이용한 방법으로, 종속변수가 범주형 변수일 때 사용 가능하다.

- **독립변수가 연속형이면서** **종속변수가 범주형 변수**일 때 사용가능하다.
- 독립변수가 어떤 값을 가지든 종속변수는 각 범주에 포함된 **확률값**을 반환하여 분류한다.

로지스틱 회귀분석의 알고리즘

- **오즈**(Odds) : 성공할 확률이 실패할 확률의 몇 배인지 나타내는 값, 집단에 분류될 확률 값을 추정
    - 음수를 가질 수 없다
    - 확률값과 오즈의 그래프는 비대칭성을 띤다
- **로짓변환** : 오즈에 로그값을 취하는 것을 말한다.
    - 확률값과 로짓값의 그래프는 0.5를 기준으로 대칭형태를 띠게된다.
- **시그모이드함수** : 로짓함수와 역함수 관계, 확률 값을 중심으로 로지스틱 회귀분석식을 정리
    
    ![https://steemitimages.com/640x0/https://cdn.steemitimages.com/DQmZPt84zvCcHSdsq9K2E5Rq35tB9imazNJhkpkheomQRn4/noname06.png](https://steemitimages.com/640x0/https://cdn.steemitimages.com/DQmZPt84zvCcHSdsq9K2E5Rq35tB9imazNJhkpkheomQRn4/noname06.png)
    

---

**의사결정나무**는 자료를 학습하여 특정 분리 규칙을 찾아내고, 몇 개의 소집단으로 분류하는 분석방법이다.

- 의사결정이 진행되는 방식을 한눈에 볼 수 있다.
- 구성요소
    - 자식마디 : 하나의 마디로 부터 나온 2개 이상의 하위 마디
    - 부모마디 : 자식마디의 바로 상위 마디
    - **뿌리마디** : **가장 최상위 마디**
    - 중간마디 : 부모마디와 자식마디를 모두 보유한 마디
    - **끝마디** : **가장 최하위 마디**
    - 가지 : 마디를 서로 연결하는 연결선
    - 깊이 : 뿌리마디를 제외한 중간마디수
- 의사결정나무 활용
    - **세분화** : 비슷한 특성을 가진 그룹별로 분할
    - **분류** : 종속변수의 범주를 몇 개의 등급으로 분류
    - **예측** : 데이터들로부터 규칙을 찾아내어 예측에 활용
    - **차원 축소 및 변수 선택** : 가장 큰 영향을 끼치는 독립변수를 선택
    - **교호작용** : 여러 독립변수들을 결합하여 종속변수에 작용하는 규칙 파악 / 범주형 변수 병합 또는 연속형 변수를 이산화
- 모델이 **직관적**이고 **해석이 용이**하다, **이상값에 민감하지 않다, 전처리가 쉽다**
- **독립변수들 사이 중요도 판단이 어렵다**, 분류 **경계선 근처의 자료에 대해 오차가 크다**, **과적합** 가능성이 높다.

---

의사결정나무의 분석 과정

1. 성장 : 분석 목적과 자료구조에 따라 
    - **분리 기준** : 불순도 사용, 분류가 잘 되어있으면 **다양한 범주의 데이터로 구성되면 불순도가 크다.**
        - 종속변수가 이산형 : 카이제곱 검정, 지니 지수, 엔트로피 지수
        - 종속변수가 연속형 : 분산분석에서의 F-통계량, 분산감소량
            - 지니지수 : 지니지수가 높을수록 데이터는 분산되어있음 → 지니지수가 가장 작은 변수를 기준으로 자식 마디 생성
            
            $$
            G(S) = 1-\sum_{i=1}^c p_i^2
            $$
            
            - 카이제곱 통계량 : p-value가 가장 적은 변수를 기준으로 자식마디 생성
                
                ![https://velog.velcdn.com/images/khyun11/post/216a9869-4c31-4b20-8bda-2e031f764443/image.png](https://velog.velcdn.com/images/khyun11/post/216a9869-4c31-4b20-8bda-2e031f764443/image.png)
                
    - **정지 규칙** : 해석상의 어려움을 막기위해 특정 조건하 더 이상 분리가 일어나지 않도록 정지
2. 가지치기
    - 모형이 너무 복잡해 과적합이 발생할 경우 일부 가지를 적당히 제거
3. 타당성 평가
    - 모델의 예측 정확도 평가, 평가지표를 이용해 의사결정나무 평가
4. 해석 및 예측

---

**앙상블 분석** : 여러 번의 데이터 분할을 통하여 구축된 다수의 모형을 결합하여 새로운 모형을 만드는 방법

- 배깅은 여러 개의 붓스트랩을 집계하는 알고리즘이다.
    - 각각의 모델을 분류기(classifier)라고 부르며, 의사결정 모델을 사용한다.
    - 다수결에 의하여 최종 결과값ㅇ르 선정하는 작업을 보팅(voting)이라고 한다.
    - 데이터 모집단의 분포를 현실적으로 알 수 는 없지만 글동안 알 수 없던 모집단의 특성을 더 잘 반영할 수 있다.
- 부스팅은 이전 모델을 구축한 뒤 다음 모델을 구축할 때 이전 분류기에 의해 잘못 분류된 데이터에 더 큰 **가중치**를 주어 붓스트랩을 구성한다.
    - 약한 모델을 결합함으로써 점차적으로 강한 분류기를 만들어 나간다.
    - 훈련오차를 빠르게 줄일 수 있고, 배깅보다 예측 성능이 뛰어나다.
- 랜덤 포레스트는 배깅에서 더 많은 무작위성을 주는 분석기법이다.
    - 붓스트랩에서 한번 더 표본추출과정을 반복하여 추출된표본에 대해 분할을 실시
    - 일반화 성능이 향상되고 이상값에 민감하지 않다.

---

**인공신경망**은 인간의 뇌를 수학적으로 단순호해 모델링한 것이다.

- 값이 입력되면 개별 신호의 정도에 따라 값이 가중되고, 그 값에 편향이라는 상수를 더한후 활성함수를 거쳐 출력값이 생성된다.
- 잡음의 영향이 적고 비선형적 문제, 패턴인식, 예측 등에 효과적이다, 스스로 가중치를 학습한다.
- 학습에 시간이 오래 걸리고 해석이 쉽지않다, 가중치의 신뢰도가 낮다.

인공신경망은 노드에 입력되는 값을 비선형 함수에 통과시킨 후 다음 노드에 전달하는데, 사용되는 비선형함수를 **활성함수**라고 한다.

- 사용하는 활성함수에 따라 출력값이 달라짐
- Step 함수 :  0 또는 1을 반환
- Sigmoid 함수 : 0과 1사이의 값을 반환한는 연속함수
- Sign 함수 : -1 또는 1을 반환
- tanh 함수 : 확장된 형태의 시그모이드 함수, -1에서 1사이의 값을 출력
- ReLU 함수 : 입력값과 0 중에서 큰 값을 반환
- Softmax 함수(표준화지수 함수) : 출력값이 다범주인 경우 사용, 각 범주에 속할 확률값을 반환

인공신경망의 계층 구조

- **입력층** : 데이터를 입력받아 시스템으로 전송
- **은닉층** : 신경망 외부에서 접근할 수 없도록 숨겨진 층
    - 입력층으로부터 값을 전달 받아 가중치를 계산한 후 활성함수에 적용하여 결과를 산출하여 출력층으로 보냄
- **출력층** :  학습데이터가 포함된 활성함수의 결과를 담고있는 층

인공신경망은 여러개의 퍼셉트론으로 구성, 퍼셉트론이 보유한 여러 개의 **가중치 값을 결정**하는 것이 중요

가중치 값의 결정

- 순전파 알고리즘 : 입력층에서 출력층으로 찾아 나감
- 역전파 알고리즘 : 순전파 알고리즘에서 발생한 오차들을 줄이기 위해 출력층에서 입력층으로 거꾸로 찾아나감

인공신경망의 종류

- **단층 퍼셉트론(단층 신경망)** : **다수의 입력층**이 바로 **하나의 출력층**으로 연결
    - 퍼셉트론의 출력값은 또 다른 퍼셉트론의 입력 데이터가 된다.
- **다층 퍼셉트론(다층 신경망)** : 입력층과 출력층 사이의 다수의 은닉층을 가지고 있음
    - 적절한 노드 수를 찾는 것이 중요하다.

---

베이즈 이론(베이지안 확률) : 확률을 해석하는 이론

- 빈도확률이 객관적으로 확률을 해석한다면, 베이지안 확률은 주관적으로 확률을 해석
- 사전확률과 우도확률을 통해 사후확률을 추정
- 분석자의 사전지식, 주관까지 포함해 분석하는 방법

나이브 베이즈 분류모델은 베이즈 정리를 기반으로 한 지도학습 모델이다.

- 모든 특징 변수가 서로 동등하고 독립적이라는 가정하에 분류를 실행한더.

나이브 베이즈 알고리즘은 이진 분류 데이터가 주어졌을 때 베이즈 이론을 통해 범주 a,b 가 될 확률을 구하고, 더 큰 확률값이 나오는 범주에 데이터를 할당

---

**k-NN 알고리즘 :** 정답 라벨이 없는 자신의 데이터를 분류하기 위해 정답라벨이 있는 주변의 데이터들을 분석해서 가장 가까이에 있는 데이터의 라벨을 확인

- 분류 분석에 속하지만 군집의 특성도 가지고 있다.
- 최적의 k값을 정하는 것이 관건이다. 일반적으로는 총 데이터들의 제곱근 값을 사용

---

서포트벡터머신(SVM) : 초평면을 이용하여 카테고리를 나누어 비확률적 이진 선형모델을 만듬

- 가장 높은 마진을 가져가는 뱡향으로 분류하여 분류성능이 뛰어남
- 좋은 분류를 위해서는 가장 가까운 학습 데이터와 가장 먼 거리를 가지는 초평면을 찾아야함

---

분류 모형의 기준에는 일반화, 효율성, 분류 정확성 등의 기준이 있다.

**오분류표**는 실제값과 예측치의 값에 대한 옳고 그름을 표로 나타낸 것

[https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbdJdL0%2FbtqZefsbGyY%2FMN1mWtcXE6tkjziy31fJTk%2Fimg.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbdJdL0%2FbtqZefsbGyY%2FMN1mWtcXE6tkjziy31fJTk%2Fimg.png)

**TP**(True Positive) : 예측한 값이 positive이고 실제 값도 positive인 경우

**FP**(False Positive) : 예측한 값이 positive이고 실제 값은 negative인 경우

**TN**(True Negative) : 예측한 값이 negative이고 실제 값도 negative인 경우

**FN**(False Negative) : 예측한 값이 negative이고 실제 값은 positive인 경우

**정분류율** = 정확도 : 전체 관측치 중 올바르게 예측한 비율

$$
\frac {TP + TN} {TP + FP + FN + TN}
$$

**오분류율** : 전체 관측치 중 잘못 예측한 비율

$$
\frac{FN + FP}{TP + FN+FP+TN}
$$

**민감도** = 재현율 : **실제 True 중 올바르게 True를 찾아낸 비율**, 모형의 **완전성** 평가지표

$$
\frac {TP}{TP+FN}
$$

**특이도** : 실제 False 중 올바르게 False를 찾아낸 비율

$$
\frac{TN}{TN+FP}
$$

**정밀도** : 예측 True 중 올바르게 True를 찾아낸 비율

$$
\frac{TP}{TP+FP}
$$

**F1 Score** : 정밀도와 재현율의 조화평균 값, 값이 높을수록 좋다

$$
\frac{2*정밀도*재현율}{정밀도+재현율}
$$

**거짓 긍정률(FPR)** : 실제 negative 값 중 positive 로 잘못 분류한 비율

 

$$
1- \frac {TN}{FP+TN} =\frac{FP}{FP+TN}
$$

---

**ROC 커브**는 분류 분석 모형의 평가를 쉽게 비교할 수 있도록 시각화한 그래프이다.

x축은 FPR 값을 , y축은 TPR(민감도)  값을 갖는다.

ROC 커브 아래 면적(AUROC) 값이 **1에 가까울수록 모형성능이 우수**하며,

0.5에 가까울수록 좋지못한 모형이다.

---

**이익도표**는 모델의 성능을 판단하기 위해 작성한 표이다.

목표범주에 속할 확률을 내림차순으로 정렬하여 구간을 나눈 후 각 구간에서의 성능을 판단한다.

랜덤모델의 예측력 = 목펴범주 그룹 1에 속한 데이터 개수 / 전체 데이터 개수

향상도 = 반응률 / 랜덤모델의 예측력

---

**향상도 곡선**은 해당 모델의 성과가 얼마나 향상되었는지 구간별로 파악하기 위한 그래프이다.

좋은 모델일수록 큰값에서 시작해 급격히 하락한다.